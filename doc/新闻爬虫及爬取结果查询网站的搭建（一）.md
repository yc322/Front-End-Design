@[TOC](新闻爬虫及爬取结果查询网站的搭建(一）
# 实验要求
## 核心需求

1、选取3-5个代表性的新闻网站（比如新浪新闻、网易新闻等，或者某个垂直领域权威性的网站比如经济领域的雪球财经、东方财富等，或者体育领域的腾讯体育、虎扑体育等等）建立爬虫，针对不同网站的新闻页面进行分析，爬取出编码、标题、作者、时间、关键词、摘要、内容、来源等结构化信息，存储在数据库中。
2、建立网站提供对爬取内容的分项全文搜索，给出所查关键词的时间热度分析。

## 技术要求
1、必须采用Node.JS实现网络爬虫
2、必须采用Node.JS实现查询网站后端，HTML+JS实现前端（尽量不要使用任何前后端框架）


# 爬虫准备工作
一共选了三个新闻网站进行爬虫，分别是**中国财经网**、**雪球网**、**东方财富网**，并且将爬取结果存储在postgresql中。
在本实验中，基于Node.js用Cheerio和Request实现了爬虫。下面将详细介绍基本环境搭配，各个爬虫的实现，功能实现过程等内容。

## Node.js 安装配置
node.js官网：[https://nodejs.org/zh-cn/](https://nodejs.org/zh-cn/)
安装非常简单，顺着点一点就好了。
## vscode
 Visual Studio Code（以下简称vscode）是一个轻量且强大的跨平台开源代码编辑器（IDE），支持Windows，OS X和Linux。内置JavaScript、TypeScript和Node.js支持，而且拥有丰富的插件生态系统，可通过安装插件来支持C++、C#、Python、PHP等其他语言。
在本实验中，我使用的是vscode对node.js进行调试。
可以参考官网：[https://code.visualstudio.com/](https://code.visualstudio.com/)

## Request
Request也是一个Node.js的模块库，可以轻松地完成http请求。
1. 安装
**npm install request**
2. 基本使用方法：
在本实验中，主要是通过request来获取新闻页面。主要使用其默认的GET方法。
```javascript
var request = require('request');
request('url', function (error, response, body) {
  if (!error && response.statusCode == 200) {
    console.log(body) // 请求成功的处理逻辑
  }
});
```

## Cheerio简介
- Cheerio介绍与安装
cheerio是jquery核心功能的一个快速灵活而又简洁的实现，主要是为了用在服务器端需要对DOM进行操作的地方。
cheerio官网: [https://cheerio.js.org/](https://cheerio.js.org/).
也可以参考中文翻译：[https://www.jianshu.com/p/629a81b4e013](https://www.jianshu.com/p/629a81b4e013)
安装：在项目目录下执行 npm install cheerio
- 主要功能
在本次实验中，主要用到的功能将在本节进行一个简单介绍。
1. 解析HTML(load)
  首先需要手动加载html文档，使用的方式如下，其他更多的加载方式可以参考官方文档
	 ```javascript
		var cheerio = require('cheerio'),
		$ = cheerio.load('<ul id = "fruits">...</ul>');
	 ```
 3. 选择器（selecter)
 cheerio选择器几乎和jQuery一模一样。选择器是文档遍历和操作的起点。如同在jQuery中一样，它是选择元素节点最重要的方法，但是在jQuery中选择器建立在CSS选择器标准库上。
在本实验中，主要用到了以下几种选择器方法。
	选取页面中所有的超链接。首先获取页面中所有\<a\>标签，再遍历获取其href属性的值，具体的局部代码如下：
	```javascript
	try {
	        seedurl_news = $('a');
	    } catch (e) { console.log('url列表所处的html块识别出错：' + e) };
	    seedurl_news.each(function(){
	        try {
	            var href = "";
	            href = $(this).attr('href'); //获取href属性
	        }catch(e) {
	            console.log('get the seed url err' + e);
	        }
	    }
	```
	按照指定class属性名称来进行选取，注意在class name中间有空格的时候，选取时改为., 示例代码如下：
	```javascript
	 $('.source.data-source')
	```
	如果要选取某一个标签下，其子标签的内容，但他的子标签并没有唯一的class name 或者id相对应，可以使用.children( selector )来选择其子节点，并且eq（）是选择第几个子节点，从0开始。示例代码如下:
	```javascript
	$('.article-meta').children().eq(1).text(); 
	```
## 数据库存储
- 概述
PostgreSQL是一种特性非常齐全的自由软件的对象-关系型数据库管理系统。
在本实验中，将爬虫得到的数据存储在PostgreSQL中，并且在爬取页面时，对该url是否爬取过进行检查。
- 数据库安装
可以在官网上进行下载，官网连接[https://www.postgresql.org/](https://www.postgresql.org/)
在windows系统上的安装教程可以参考[https://www.yiibai.com/postgresql/install-postgresql.html](https://www.yiibai.com/postgresql/install-postgresql.html)

- nodejs连接pg数据库
 nodejs连接pg数据库有两种方式：
一种是直接连接、操作、断开。
一种是使用**连接池**，这种方式可以有效提升多并发的效率。
在本实验中，采用的是连接池的方法连接数据库。
首先使用npm安装数据库连接模块：
	```
	npm install pg
	```
	**连接池创建**
	在代码中引入pg模块，并编写数据库配置：
	```javascript
	var pg = require('pg');
	
	// 数据库配置
	var config = {  
	    user:"postgres",
	    database:"Spider",
	    password:"postgres",
	    port:5432,
	    // 扩展属性
	    max:20, // 连接池最大连接数
	    idleTimeoutMillis:3000, // 连接最大空闲时间 3s
	}
	```
	创建连接池：
	```javascript
	// 创建连接池
	var pool = new pg.Pool(config);
	```
	在本实验中，主要是对数据库进行查询和插入的操作，并没有用更新和删除操作。
	在这里主要写了两个函数，用于操作有参数和无参数的情况。
有参数
	```javascript
	var query = function(sql, sqlparam, callback) {
	    pool.connect(function(err, conn, done) {
	        if (err) {
	            console.log(err)
	            callback(err, null, null);
	        } else {
	            conn.query(sql, sqlparam, function(err, result) {
	                //conn.release(); //释放连接 
	                done();
	                callback(err, result); //事件驱动回调 
	            });
	        }
	    });
	};
	```
	无参数
	```javascript
	var query_noparam = function(sql, callback) {
	    pool.connect(function(err, conn, done) {
	        if (err) {
	            callback(err, null, null);
	        } else {
	            conn.query(sql, function(err, result) {
	                conn.release(); //释放连接 
	                callback(err, result); //事件驱动回调 
	            });
	        }
	    });
	};
	```
- 新闻在数据库中的存储格式
在pgSQL中，运行以下语句，建立存储了url，来源，编码，标题，作者，爬取时间，新闻发布时间，内容，分类，和阅读次数的数据存储schema。
	```
	CREATE TABLE fetches (
	   id_fetches serial UNIQUE,
	   url text DEFAULT NULL UNIQUE,
	   source_name varchar(50) DEFAULT NULL,
	   source_encoding varchar(45) DEFAULT NULL,
	   title varchar(100) DEFAULT NULL,
	   author varchar(100) DEFAULT NULL,
	   crawltime date DEFAULT CURRENT_TIMESTAMP,
	   publish_date date DEFAULT CURRENT_TIMESTAMP,
	   content text,
	   category text DEFAULT NULL,
	   read_num integer DEFAULT NULL,
	  PRIMARY KEY (id_fetches)
	);
	```
- 查看url 是否已经爬取过
  将爬取数据进行存储的其中一个目的是，防止重复爬取同一页面。
  查询返回的result是一个对象，其中rows是一个存储列名和值的数组，因此判断url是否重复的时候，是判断result.rows[0]是否为null。
  
	```javascript
	// not try to crawl the repeat page
       var fetch_url_Sql = 'select url from fetches where url= $1';
       var fetch_url_Sql_Params = [href];
       pgsql.query(fetch_url_Sql, fetch_url_Sql_Params, function(err, result) {
           if (err) {
               console.log(err)
           } else { // a new page
               if(result.rows[0] != null) {
                   console.log(result.rows[0]);
                   console.log("URL " + href + " repeat");
               } else {
                   newsGet(href);
               }
           }
     });
	```
- 将爬虫的结果存储到数据库中
	```javascript
	// save the result in postgresql
	var fetchAddSql = 'INSERT INTO fetches (url, source_name, source_encoding, title, author, publish_date, content, read_num)' 
	+ 'VALUES ($1, $2,$3,$4,$5, $6, $7,$8)';
	var fetchAddSql_Params = [fetch.url, fetch.source, fetch.source_encoding,
	    fetch.title, fetch.author, fetch.publish_date,
	    fetch.content, fetch.read_num
	];
	
	//执行sql，数据库中fetch表里的url属性是unique的，不会把重复的url内容写入数据库
	pgsql.query(fetchAddSql, fetchAddSql_Params, function(err, result) {
	    if(err) {
	        console.log(err);
	    }
	});
	```
